# SwiftKey Natural Language Processing (NLP) Project

## Overview
In this project, a 4-gram language model is implemented to make next-word predictions (autocomplete), given a certain input phrase. The language model was initially trained with text data from news articles, various blogs and Twitter. The model makes predictions using the Markov assumption that the next word can be predicted only using the previous 3 words in a phrase, followed by Maximum Likelihood Estimation (MLE). To deal with unknown n-grams at prediction-time, a modified version of a technique known as ["stupid backoff"](https://aclanthology.org/D07-1090.pdf) is applied, "backing off" to (n-1)-grams and carrying out MLE (with a discounting factor) using (n-1)-grams to predict the next word. Additionally, a sentence-wise term co-occurrence matrix is naively incorporated into the model to provide context to the predictions, thereby enhancing them slightly. Ultimately, the prediction algorithm has only achieved an approximately 11% sentence completion accuracy on limited tests, likely due to using only a fraction of the available training data ("stupid backoff" has been shown to word well on large datasets, but local memory limitations have not allowed for this at the time-being). Future implementations may: 

1. Use Spark for processing power and better training
2. Implement a more probabilistically robust smoothing (like Kneser-Ney smoothing) and a similarly robust incorporation of the term co-occurrence matrix
3. Implement a recurrent neural network for predictions


## File Structure

- `/app`: Contains code for the [Shiny application](https://akota64.shinyapps.io/Autocomplete_Application) on which the model is deployed. Also contains data used in the application, but generated by the modeling notebook in the `/model` directory.
  - `/app/data`: A copy of `/model/data` to be used for prediction within the web application
- `/data`: Contains processed data used in the model. These processed data are produced by scripts in the `/scripts` directory, as well as the modeling notebook located in the `/model` directory.
- `/eda`: Contains an exploratory analysis R markdown document, which is presented as a milestone report of sorts. This document has also been published on [RPubs](https://rpubs.com/imak64/932787).
- `/model`: Contains the main modeling notebook where all modeling, training, and validation has been carried out. The notebook documents the exact model used and process of arriving at the model in great detail. This notebook is also published [here](https://rpubs.com/imak64/935112).
  - `/model/data`: Contains the final output data objects from the model, which are used in preidction in `/model/final_model.R`. Note that not all data used for prediction has been committed, due to size (specifically, the term co-occurrence matrix is not available here)
- `/pitch`: Contains an R Markdown presentation giving a complete overview of the application and the methods used in the model. This has been published [here](https://rpubs.com/imak64/933651).
- `/scripts`: Contains general scripts that have been used to initially pre-process/parse the provided text data, using the tidytext library


## Dataset Information
The dataset used in this project has been kindly provided by [SwiftKey](https://www.microsoft.com/en-us/swiftkey?activetab=pivot_1%3aprimaryr2), a subsidiary of Microsoft that specializes in their namesake smart keyboard product. The dataset contains text data separated by language (US English, German, Russian, and Finnish) and by source (news, blogs, Twitter). The data is presented as lines in simple .txt files, and the total data set after de-compression comes out to be 1.41 GB. Click on [this link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) to download the full dataset provided by SwiftKey. A sample of 5% of this data is used, with 72% of this sample being used for training, 8% for initial cross-validation ("held-out"), 10% for final cross-validation, and 10% for testing.

## Model Design
### 4-gram Language Model
The 4-gram language model is a simple n-gram model which assumes that one can predict the next word in a given phrase using only the previous 3 words. By MLE using our dataset, the probability of the next word in a phrase being $w_n$, given the previous (n-1) words were $w_{1:(n-1)} = w_1, \dots, w_{n-1}$ is given by the equation:
$$P(w_n|w_{1:(n-1)}) = \frac{C(w_{1:n})}{C(w_{1:(n-1)})}$$
where $C(w_{1:n})$ represents the total number of times we have seen the n-gram $w_{1:n}$ in our dataset [1]. Using count matrices of all 3-grams and 4-grams in the training dataset, we are able to calculate a conditional probability matrix of each 4-gram $w_{1:4}$, given the initial 3-gram $w_{1:3}$. For prediction, the 4-gram probability matrix can filtered by setting the last 3 words of a given phrase as $w_{1:3}$, and the 4th word $w_4$ with the highest probability of occurring can be chosen as the prediciton.

### Modified "Stupid Backoff"
The main downside of model above is the inability to deal with out-of-vocabulary (OOV) 4-grams. If any of the last 3 words of a phrase have not been seen in the training data set, the model will simply not produce a prediction. The way that this model works around that is by a method known as "stupid backoff" [2], where the prediction algorithm backs off to 3-grams, and tries to predict the next word with only the last 2 words instead. This paradigm is actually used to estimate the probability of a certain 4-gram, but we have adjusted it for our purposes. The calculated probability is discounted at each backoff in the following way, given $P(w_i|w_{i-n+1:i-1})$ is unknown:
$$S(w_i|w_{i-n+1:i-1}) = \lambda P(w_i|w_{i-n+2:i-1})$$
We see that this probability no longer corresponds to a true probability, so it is denoted as $S$. We will refer to this as the conditional pseudo-probability. In our edition of this method, we always backoff all the way from 4-gram conditional probabilities to 1-gram psuedo-probabilities. At each backoff stage, we generate a prediction based on the conditional pseudo-probabilities at that stage. Thus, we end up with a 4-gram-based prediction, 3-gram-based prediction, 2-gram-based prediction, and 1-gram-based prediction, along with their respective conditional pseudo-probabilities. At the end, the word prediciton $w_i$ with the highest conditional pseudo-probability is chosen as our final prediction. Using the discounted pseudo-probabilities at this final selection stage helps to give the 4-gram model higher precedence over the lower-order n-gram models, which is how the prediction should theoretically work best (given enough training data). Through parameter tuning on the held-out set, we have found the optimal $\lambda$ to be $\lambda = 0.55$.

### Term Co-Occurrence Matrix


## References

[1] Jurafsky, D., &amp; Martin, J. H. (2014). Speech and language processing. Pearson. 
[2] [Large Language Models in Machine Translation](https://aclanthology.org/D07-1090) (Brants et al., EMNLP 2007)
