# SwiftKey Natural Language Processing (NLP) Project

## Overview
In this project, a 4-gram language model is implemented to make next-word predictions (autocomplete), given a certain input phrase. The language model was initially trained with text data from news articles, various blogs and Twitter. The model makes predictions using the Markov assumption that the next word can be predicted only using the previous 3 words in a phrase, followed by Maximum Likelihood Estimation (MLE). To deal with unknown n-grams at prediction-time, a modified version of a technique known as ["stupid backoff"](https://aclanthology.org/D07-1090.pdf) is applied, "backing off" to (n-1)-grams and carrying out MLE (with a discounting factor) using (n-1)-grams to predict the next word. Additionally, a sentence-wise term co-occurrence matrix is naively incorporated into the model to provide context to the predictions, thereby enhancing them slightly. Ultimately, the prediction algorithm has only achieved an approximately 11% sentence completion accuracy on limited tests, likely due to using only a fraction of the available training data ("stupid backoff" has been shown to word well on large datasets, but local memory limitations have not allowed for this at the time-being). Future implementations may: 

1. Use Spark for processing power and better training
2. Implement a more probabilistically robust smoothing (like Kneser-Ney smoothing) and a similarly robust incorporation of the term co-occurrence matrix
3. Implement a recurrent neural network for predictions


## File Structure

- `/app`: Contains code for the [Shiny application](https://akota64.shinyapps.io/Autocomplete_Application) on which the model is deployed. Also contains data used in the application, but generated by the modeling notebook in the `/model` directory.
  - `/app/data`: A copy of `/model/data` to be used for prediction within the web application
- `/data`: Contains processed data used in the model. These processed data are produced by scripts in the `/scripts` directory, as well as the modeling notebook located in the `/model` directory.
- `/eda`: Contains an exploratory analysis R markdown document, which is presented as a milestone report of sorts. This document has also been published on [RPubs](https://rpubs.com/imak64/932787).
- `/model`: Contains the main modeling notebook where all modeling, training, and validation has been carried out. The notebook documents the exact model used and process of arriving at the model in great detail. This notebook is also published [here](https://rpubs.com/imak64/935112).
  - `/model/data`: Contains the final output data objects from the model, which are used in preidction in `/model/final_model.R`. Note that not all data used for prediction has been committed, due to size (specifically, the term co-occurrence matrix is not available here)
- `/pitch`: Contains an R Markdown presentation giving a complete overview of the application and the methods used in the model. This has been published [here](https://rpubs.com/imak64/933651).
- `/scripts`: Contains general scripts that have been used to initially pre-process/parse the provided text data, using the tidytext library


## Dataset Information
The dataset used in this project has been kindly provided by [SwiftKey](https://www.microsoft.com/en-us/swiftkey?activetab=pivot_1%3aprimaryr2), a subsidiary of Microsoft that specializes in their namesake smart keyboard product. The dataset contains text data separated by language (US English, German, Russian, and Finnish) and by source (news, blogs, Twitter). The data is presented as lines in simple .txt files, and the total data set after de-compression comes out to be 1.41 GB. Click on [this link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) to download the full dataset provided by SwiftKey.

## Model Design
### 4-gram Language Model

### Modified "Stupid Backoff"

### Term Co-Occurrence Matrix
