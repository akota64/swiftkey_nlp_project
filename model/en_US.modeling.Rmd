---
title: "Modeling Notebook: English Language Data"
output: html_notebook
---

## Overview
In this notebook, we investigate various language models and their effectiveness in fitting the training and held-out data. 

## Initial Considerations
The possible choices for language models are vast, but for the purposes of this project, we will first look in to the probabilistic Markov-based language model using count and probability matrices. The main weakness in this technique is dealing with out-of-vocabulary (OOV) words when performing prediction, but there are a plethora of ways to address this weakness. Listed below are some of the most common:

- Unknown Word Tagging: Replacing unknown words with <UNK>. This method can have the effect of artificially reducing perplexity scores if too many words are <UNK>.
- Additive Smoothing: Includes Laplace variation and add-k variation. This method smooths the n-gram probability distribution, but can have drastic effects on sparse probability distributions like language models. The non-zero probability terms tend to be significantly reduced, much more than we would like.
- Katz Backoff: A backoff method which calculates the probabilities of unseen n-grams by "backing off" to lower-order n-grams. The probability is further discounted at each backoff, so as to keep the total probability as 1. This backoff constant $\alpha$ is generally a function of the previous (n-1)-gram.
- Stupid Backoff: A backoff method that forfeits the idea of creating a sound probability distribution, and instead uses a constant backoff constant $\alpha$. Studies have shown that an $\alpha\approx0.4$ is quite effective for language models using big data sets (where other methods may not be computationally practical).
- Interpolation: Assigning probabilities of unseen n-grams as linear combinations of corresponding lower-order n-gram probabilities.
- Interpolated Kneser-Ney Smoothing: An augmentation of the absolute discounting (subtracting some set value from the non-zero counts before probability calculation) that formalizes and incorporates a continuation probability, which is the probability (based on observations) that a certain word appears as a continuation in a novel context. However, this could be too computationally intensive for 4-grams (for our purpose).
- Good-Turing Smoothing: Another smoothing mechanism that is often used, which adjusts count in a more intelligent way than additive smoothing. Can be quite computationally intensive, so we will likely not pursue this.
  
There are also several other models that may be of use in our predictions. These include:

- Part-Of-Speech Tagging: Tagging parts of speeches, which has been achieved with a relatively high accuracy to date. This can be incorporated in the probability matrix by discounting unlikely combinations part-of-speech combinations. This may be unnecessary with a large enough set of training data though.
- Co-Occurrence Matrices: Matrices that track the co-occurrence of words across different larger units of language (phrases, sentences, lines, documents, etc.). This is an interesting candidate, as it would allow us to track sentence or line context better. For example, completing the sentence "In Paris, the building that I adore the most is ___" relies on the word Paris, which is difficult to track with n-grams. Incorporating co-occurrence somehow could allow for recommending "Eiffel" as the next word.
- Sentiment Analysis with Word Embeddings: A method for vectorizing words in a sentiment space, and thus allowing us to judge the sentiment of n-grams, sentences, and lines. This could be useful, but is likely too computationally intensive, and co-occurrence matrices could probably encapsulate a good portion of this functionality.

## Chosen Method
From the initial considerations, we have chosen to dive deeper into the following model, along with the base model:

- Stupid Backoff: A computationally light method that has shown to be very effective on big data. We may also be able to use more of the training data and higher n-grams with this method.

Furthermore, we will also look into using co-occurrence matrices for model optimization. The difficulty here will be finding the optimal way to incorporate the co-occurrence results into our overall probability/pseudo-probability distribution.

## Base Model
### 1. Generate Counts
The base model will use unigram, bigram, and trigram probability matrices to predict the next word given a phrase. We first download the training set of lines and extract sentences and 1-grams through 4-grams from the data. This may take a few minutes to run.
```{r ngrams}
suppressMessages(here::i_am("model/en_US.modeling.Rmd"))
suppressPackageStartupMessages(require(here))
suppressPackageStartupMessages(require(tidytext))
suppressPackageStartupMessages(require(tidyverse))

lines <- readRDS(here("data/en_US/train_en_US_lines.Rds"))

sentences <- unnest_sentences(lines, sentence, line) %>% mutate(type_id = factor(type_id), sentence_id = row_number())

ngrams1 <- unnest_ngrams(sentences, "word1", sentence, n=1)
ngrams2 <- unnest_ngrams(sentences, bigram, sentence, n=2) %>%
    separate(bigram, c("word1", "word2"), sep = " ")
ngrams3 <- unnest_ngrams(sentences, trigram, sentence, n=3) %>%
    separate(trigram, c("word1", "word2", "word3"), sep = " ")
ngrams4 <- unnest_ngrams(sentences, quadrigram, sentence, n=4) %>%
    separate(quadrigram, c("word1", "word2", "word3", "word4"), sep = " ")
```

Now, we can start making our count and conditional probability matrices. The conditional probabilities of a particular n-gram given an (n-1)-gram are calculated to be:
$$P(w_n|w_{1:n-1}) = \frac{C(w_1 \dots w_n)}{C(w_1 \dots w_{n-1})}$$
This follows from Maximum Likelihood Estimation. For unigrams, this is simply the proportion of a specific unigram over all unigrams and is calculated very easily as a 1-D vector. For the other n-grams, we get matrices with the (n-1)-grams $w_{1:i-1}$ listed in the rows and 1-grams $w_i$ in the columns, which end up being very large matrices.

```{r unigram_counts}
ngrams1_stats <- as_tibble(ngrams1) %>%
    group_by(word1) %>%
    summarize(count = n()) %>%
    arrange(desc(count))
ngrams1_stats
```

As we see, the sample matches our full data set pretty well in terms of top words! Next, we tackle the other ngram counts.

```{r ngram_counts}
ngrams2_stats <- as_tibble(ngrams2) %>%
    group_by(word1, word2) %>%
    summarize(count = n())
ngrams2_stats
ngrams3_stats <- as_tibble(ngrams3) %>%
    group_by(word1, word2, word3) %>%
    summarize(count = n())
ngrams3_stats
ngrams4_stats <- as_tibble(ngrams4) %>%
    group_by(word1, word2, word3, word4) %>%
    summarize(count = n())
ngrams4_stats

rm(list=c("ngrams1","ngrams2","ngrams3","ngrams4"))
```

We immediately see quite a lot of profanities. Let's get rid of these we will be using the standard [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en) available on github to filter the words. These have been downloaded into the project's file system.
```{r filter_profanities}
profanities <- readLines(here("data/en_US/profanity_en_US.txt"))
ngrams1_stats <- ngrams1_stats %>% 
    filter(!(word1 %in% profanities))
ngrams2_stats <- ngrams2_stats %>% 
    filter(!(word1 %in% profanities), !(word2 %in% profanities))
ngrams3_stats <- ngrams3_stats %>% 
    filter(!(word1 %in% profanities), !(word2 %in% profanities), !(word3 %in% profanities))
ngrams4_stats <- ngrams4_stats %>% 
    filter(!(word1 %in% profanities), !(word2 %in% profanities), !(word3 %in% profanities), !(word4 %in% profanities))
```

Some profanities that are misspelled or distorted will still remain, but the majority of them should be filtered out now.  
  
### 2. Generate Probabilities
The probability matrix for unigram is simply the count of a particular unigram over the count of all unigrams. The probability matrix for other n-grams are a bit more involved. Let's take a stab at it below.
```{r ngram_probs}
ngrams4_stats <- left_join(ngrams4_stats, ngrams3_stats, by=c("word1", "word2", "word3"))
ngrams3_stats <- left_join(ngrams3_stats, ngrams2_stats, by=c("word1", "word2"))
ngrams2_stats <- left_join(ngrams2_stats, ngrams1_stats, by=c("word1"))

ngrams1_stats <- ngrams1_stats %>% 
    mutate(log_cond_prob = log10(count/sum(count)))

ngrams4_stats <- ngrams4_stats %>%
    rename(
        ngrams4_count = count.x,
        ngrams3_count = count.y
    ) %>%
    mutate(log_cond_prob = log10(ngrams4_count/ngrams3_count))
ngrams3_stats <- ngrams3_stats %>%
    rename(
        ngrams3_count = count.x,
        ngrams2_count = count.y
    ) %>%
    mutate(log_cond_prob = log10(ngrams3_count/ngrams2_count))
ngrams2_stats <- ngrams2_stats %>%
    rename(
        ngrams2_count = count.x,
        ngrams1_count = count.y
    ) %>%
    mutate(log_cond_prob = log10(ngrams2_count/ngrams1_count))
```
Note that we are using log-10 probabilities to prevent numeric underflow while calculating total probabilities. These probabilities constitute the base model! For prediction, we would:

1. Parse the prediction input into words.
2. Use the last n-1 words to filter the rows of the relevant stats table.
3. Choose the nth word with the highest conditional probability as the predicted next word.

Let's formalize this into a function.
```{r base_model_predict}
#' base_model_predict: Uses the base 4-gram probability model to predict future words
#' stats: Stats df for 4-grams
#' string: Input string to predict the next word
base_model_predict <- function(string) {
    df <- data.frame(line = string)
    trigram_rows <- unnest_ngrams(df, word, line, n=1) %>% tail(3)
    trigram <- trigram_rows$word
    pred_row <- ngrams4_stats %>%
        filter(word1==trigram[1], word2==trigram[2], word3==trigram[3]) %>%
        arrange(desc(log_cond_prob)) %>%
        head(1)
    pred_row$word4
}

str <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
base_model_predict(str)

str <- "Very early observations on the Bills game: Offense still struggling but the"
base_model_predict(str)

system.time(base_model_predict(str))
```

As we see in the second string, we have an issue with out-of-vocabulary n-grams. To address this, we will be look into Stupid Backoff in the next section. Note that the base model has already taken quite a long time, so let's first try to optimize this before pursuing this OOV remedy.

## Performance Optimization
### 1. Faster Lookup with Data.Table
This speed improvement tip comes from [this article](https://www.r-bloggers.com/20 17/03/fast-data-lookups-in-r-dplyr-vs-data-table/). Below, we see the vast difference in lookup speed between data.frame and indexeddata.table.
```{r morph_ngrams4_stats}
system.time(ngrams4_stats[ngrams4_stats$word1=="word",])
system.time(subset(ngrams4_stats, word1=="word"))
system.time(ngrams4_stats %>% filter(word1=="word"))

suppressPackageStartupMessages(require(data.table))
mod_ngrams4_stats <- data.table(ngrams4_stats)
setindex(mod_ngrams4_stats, word1)
setindex(mod_ngrams4_stats, word2)
setindex(mod_ngrams4_stats, word3)
setindex(mod_ngrams4_stats, word4)
setindex(mod_ngrams4_stats, log_cond_prob)

system.time(mod_ngrams4_stats[word1=="word"])
```
Once an index is set on a data.table, the lookup becomes extremely fast, especially compared to dplyr::filter. Let's compare lookup speed with along multiple columns.
```{r dplyr_dt}
system.time(ngrams4_stats[(ngrams4_stats$word1=="of" & ngrams4_stats$word2=="the"),])
setindex(mod_ngrams4_stats, word1, word2)
system.time(mod_ngrams4_stats[word1=="of" & word2=="the"])
``` 
The difference in lookup speed is quite remarkable. For this reason, we will be using data.table from now on for the all n-gram stats (counts and probabilities).

```{r to_data_table}
ngrams1_stats <- data.table(ngrams1_stats)
setindex(ngrams1_stats, word1)
setindex(ngrams1_stats, log_cond_prob)

ngrams2_stats <- data.table(ngrams2_stats)
setindex(ngrams2_stats, word1)
setindex(ngrams2_stats, word2)
setindex(ngrams2_stats, word1, word2)
setindex(ngrams2_stats, log_cond_prob)

ngrams3_stats <- data.table(ngrams3_stats)
setindex(ngrams3_stats, word1, word2)
setindex(ngrams3_stats, word3)
setindex(ngrams3_stats, word1, word2, word3)
setindex(ngrams3_stats, log_cond_prob)

ngrams4_stats <- data.table(ngrams4_stats)
setindex(ngrams4_stats, word1, word2, word3)
setindex(ngrams4_stats, word4)
setindex(ngrams4_stats, word1, word2, word3, word4)
setindex(ngrams4_stats, log_cond_prob)

rm("mod_ngrams4_stats")
```

Below is the modified base model with the 4-gram data.table used instead of data.frame.
```{r modified_base_model_predict}
base_model_predict <- function(string){
    df <- data.frame(line = string)
    trigram_rows <- unnest_ngrams(df, word, line, n=1) %>% tail(3)
    trigram <- trigram_rows$word
    pred_rows <- ngrams4_stats[word1==trigram[1] & word2==trigram[2] & word3==trigram[3]]
    pred_rows <- pred_rows[order(-log_cond_prob)][1]
    pred_rows$word4
}

str <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
base_model_predict(str)
system.time(base_model_predict(str))
```
The elapsed time is improved drastically compared to the previous one using the data.frame and dplyr, by several orders of magnitude in fact. This should be good enough for now.
  
## Dealing with OOV N-Grams

### 1. Modified Stupid Backoff Implementation
The stupid backoff method is extremely simple. The basic principle is to deal with OOV n-grams by backing up to (n-1)-grams and using those to estimate the n-gram probability (with some discount) instead. This method is often used for very large data sets, and has been shown to have very good results. We will implement it below, slightly modifying to calculate the predicted word from all n-gram models, but choosing the best one of those based on log-10 conditional probability (using discount at each descent, of course).
```{r stupid_backoff_predict}
stupid_backoff_predict <- function(string, lambda = 0.2) {
    df <- data.frame(line = string)
    trigram_rows <- unnest_ngrams(df, word, line, n=1) %>% 
        tail(3) %>% 
        arrange(desc(row_number()))
    trigram <- trigram_rows$word
    log_lambda <- log10(lambda)
    
    pred_rows4 <- ngrams4_stats[word1==trigram[3] & word2==trigram[2] & word3==trigram[1]]
    pred_rows4 <- pred_rows4[order(-log_cond_prob)][1]
    preds <- data.frame(word=pred_rows4$word4, prob=pred_rows4$log_cond_prob)
    
    pred_rows3 <- ngrams3_stats[word1==trigram[2] & word2==trigram[1]]
    pred_rows3 <- pred_rows3[order(-log_cond_prob)][1]
    preds <- rbind(preds, c(pred_rows3$word3, pred_rows3$log_cond_prob + log_lambda))
        
    pred_rows2 <- ngrams2_stats[word1==trigram[1]]
    pred_rows2 <- pred_rows2[order(-log_cond_prob)][1]
    preds <- rbind(preds, c(pred_rows2$word2, pred_rows2$log_cond_prob + (2*log_lambda)))
    
    pred_rows1 <- ngrams1_stats[order(-log_cond_prob)][1]
    preds <- rbind(preds, c(pred_rows1$word1, pred_rows1$log_cond_prob + (3*log_lambda)))
    
    pred_word <- preds[which(as.numeric(preds$prob) == max(as.numeric(preds$prob), na.rm = TRUE)),]$word
    pred_word
}

str <- "Very early observations on the Bills game: Offense still struggling but the"
stupid_backoff_predict(str)

system.time(stupid_backoff_predict(str))
```

This prediction does not really make much sense, but the base model does not even generate a prediction for this string, so we have made some progress in addressing OOV n-grams. Also, note the speed of the algorithm. It only took 16 ms to evaluate the best word based on all 4 n-gram models!

### 2. Tuning Lambda
Now, we will try out various lambda's on our held-out set to determine which gives the highest accuracy. Unfortunately, standard perplexity cannot be used to tune the Stupid Backoff model due to its pseudo-probabilistic nature, and the effect of lambda itself on the perplexity. Thus, we will have to prepare a set of data ourselves to evaluate the accuracy. This is done below.
```{r prepare_held_out_data}
ho_lines <- readRDS(here("data/en_US/held_out_en_US_lines.Rds"))

ho_sentences <- unnest_sentences(ho_lines, sentence, line) %>% 
    mutate(type_id = factor(type_id), sentence_id = row_number())

ho_words <- unnest_ngrams(ho_sentences, word, sentence, n=1)

ho_sentences <- ho_words %>% 
    group_by(sentence_id) %>% 
    mutate(sentence = paste(word, collapse = " ")) %>%
    select(sentence_id, sentence) %>%
    distinct()

ho_data <- ho_sentences %>%
    mutate(
        string = gsub("(.*) .*$", "\\1", sentence), 
        pred_word = gsub(".* (.*)$", "\\1", sentence)
    ) %>%
    select(-sentence)
head(ho_data)
dim(ho_data)

rm(list=rm("ho_lines","ho_sentences","ho_words"))
```
The ho_data tibble contains a list of sentences and words to predict. We will use these to tune the stupid backoff model. It is a little big though, and we won't be able to try too many different lambda in a reasonable amount of time with this much data to test. Let's try using a random 10% of this held out set for accuracy testing.
```{r ho_sample}
set.seed(readLines(here("scripts/seed.txt")))
ho_sample <- ho_data[sample(1:nrow(ho_data),nrow(ho_data)*0.1),]
head(ho_sample)
dim(ho_sample)
```
Below we test out several different lambda parameters.
```{r tune_lambda}
get_sb_accuracy <- function(l) {
    acc <- sum(sapply(ho_sample$string, stupid_backoff_predict, lambda=l)==ho_sample$pred_word)/nrow(ho_sample)
    acc
}

lambdas <- seq(0, 1, by=0.1)
acc_by_lambda <- data.frame(lambda = lambdas, accuracy = sapply(lambdas, get_sb_accuracy))
                            
acc_by_lambda
```

We see an immediate difference with and without the lambda parameter, but it seems that any lambda value greater than 0 will suffice. We will use a default lambda of 0.2 from now onwards.
```{r lambda}
tuned_lambda <- 0.2
```

## Co-Occurrence Matrices

### 1. Model
Co-occurrence matrices are often used in NLP for the purpose of context and sentiment analysis. In this model we will use these matrices to help guide the n-gram algorithm to the right choice of word using prior sentence context. To calculate co-occurrence matrices on our training set, we use the textmineR package in R. Co-occurrence will be tracked on a sentence-by-sentence basis (in other words, our window is 1 sentence). Only words that are not stop words will be tracked for co-occurrence, since we will be using co-occurrence for context anyways.
```{r generate_co_occurrence matrix}
suppressPackageStartupMessages(require(textmineR))

## Term co-occurrence matrix generated from document-term matrix
dtm <- suppressWarnings(CreateDtm(sentences$sentence, stopword_vec = stop_words$word))
co_mat <- Dtm2Tcm(dtm)
rm(dtm)
dim(co_mat)
```
As we can see, our matrix is absolutely massive, with 18.1B entries. However, this matrix is sparse, and so it can be compressed into a nice column format. For this type of sparse matrix (dgCMatrix), this is done with a simple summary call.
```{r column_form_co_ocurrence}
co <- summary(co_mat)
dim(co)
head(co)
co_names <- rownames(co_mat)
head(co_names)
```
This new dense co-occurence "matrix" has only 49.1M entries, which is significantly more manageable already. We will have to track the word ID's (i,j in the matrix) in a separate vector though, which is the job of co_names. Interestingly, this dense matrix representation is actually larger in object size than the sparse matrix representation, which shows the efficiency of dgCMatrix. The reduction in entry number makes it more practical for us to manipulate and process the data though.  
  
We can transform the matrix to a data table now for ease, and rename the columns.
```{r pre_proc_co}
co <- data.table(co)

setnames(co, "i", "word1")
setnames(co, "j", "word2")
setnames(co, "x", "freq")

co$word1 <- co_names[co$word1]
co$word2 <- co_names[co$word2]

setindex(co, word1)
setindex(co, word2)
setindex(co, word1, word2)
indices(co)
```

Now checking the look up time for the co-occurrence matrix, we get:
```{r co_check}
system.time(co[word1=="word"])
```

which is fantastic! We will now move on to creating some kind of procedure using this matrix to assist our predictions.  
  
The procedure will go as follows:

1. User input will be parsed into sentences. Only the last unfinished sentence will be used.
2. The last sentence will be tokenized into words, and stop words will be removed. The remaining words will be referred to simply as words from now on. We will call these words $w_i$, so we have:
$$w_1, \dots , w_n$$
3. For each of the words, we will extract the non-zero co-occurrences and quantify the co-occurrences by proportion of total sentences containing the word that also contains the other word, a co-occurrence rate. Basically, it is the empirical probability of one word being in the sentence given some other word is already in the sentence. These will be denoted:
$$P_{ij} = P(w_j \text{ in sentence}|w_i \text{ in sentence})$$
4. These probabilities will simply be multiplied across all of the words in our sentence, which amounts to adding the log-probabilities, and then the log probabilities will be divided by the total number of words in our sentence. This will be referred to as the co-occurrence score $C_j$. Multiplication (adding logs) is used to amplify this score for words that frequently occur together, keeping them near 1 (near 0 for logs). For example, let's say we have 2 words in our sentence: "brother" and "mother". Let's say our vocabulary has 2 words that co-occur with "brother", "sister" and "friend", while "sister" and "father" co-occur with "mother". The co-occurrence score for sister is calculated as the log-probability of occurring with brother plus the log-probability of occurring with mother. The other co-occurrences are ignored. We would only like to retain words that have previously co-occurred with each of the words in the matrix, and ignore novelties. (Novelties can be accounted for in a future implementation with Interpolated Kneser-Ney Smoothing)
5. The co-occurrence score will be incorporated into the final model's log-probabilities only for those words that already work with the given n-grams. This means that we will not be considering novelties (strings that are not already in our training set) just based on context, but instead, incorporating context into our existing models. The pursuit of novelty prediction should also come with other grammatical training methods (like POS tagging) which may be incorporated in the future.  
The co-occurrence score will be incorporated into the log_cond_probs of each of the stats tables in the following way, creating a new score variable $S$ to denote the new quantity (which is clearly no longer a probability):
$$S = log(S_{sb}(w_j|w_{1:n-1})) + \beta C_j$$
for those $w_j$ that have a co-occurrence score, and
$$S = log(S_{sb}(w_j|w_{1:n-1})) + \eta$$
for those that do not. To reduce the number of parameters we have to train, we will dynamically set
$$\eta = min(\beta C_j)-\beta^2$$
and tune $\beta$ accordingly. Here, $S_{sb}$ denotes the discounted conditional log-10 probability that comes from stupid backoff, which itself can no longer be considered as a probability (hence the use of $S$ instead of $P$).

This procedure is now implemented below.
```{r get_co_occurrence_scores}
get_co_scores <- function(string){
    words <- unnest_ngrams(data.frame(line=string), word, line, n=1)$word
    words <- words[!(words %in% stop_words$word)]
    
    if(length(words)==0){
        dt <- data.table(data.frame(word2 = "", co_score = NA))
    } else {
        dt <- co[word1==words[1]][,!"word1"]
        dt$freq <- log10(dt$freq/max(dt$freq, na.rm = TRUE))
        setnames(dt, "freq", "log_prob1")
        
        for(i in 2:length(words)){
            x <- co[word1==words[i]]
            x$freq <- log10(x$freq/max(x$freq, na.rm = TRUE))
            
            dt <- merge(dt, x, by="word2")[,!"word1"]
            setnames(dt, "freq", paste0("log_prob",i))
        }
            
        dt <- dt[, co_score:=rowSums(.SD)/(ncol(dt)-1), .SDcols=2:ncol(dt)][,c("word2","co_score")]
    }
    dt
    
}

get_co_scores("My favorite thing about Paris, France is")
system.time(get_co_scores("My favorite thing about Paris, France is"))
```
We understand that this is a naive implementation, and that a more sound implementation would involve retaining all words that match at least one of the sentence words, instead of all of them, and simply penalizing every time these words don't match well with a word in the sentence. However, this naive implementation will be used for now. A later release of the prediction algorithm could include a better method to incorporate co-occurrences.  
  
The last step is to incorporate the co-occurrence results with the stupid backoff method. This is done below.
```{r full_model}
predict_next_word <- function(string, lambda = 0.2, beta = 0.55) {
    co_scores <- suppressWarnings(get_co_scores(string))
    eta_co_score <- suppressWarnings(min(co_scores$co_score, na.rm = TRUE)) - beta
    if(eta_co_score==Inf){eta_co_score <- -3}
    trigram_rows <- unnest_ngrams(data.frame(line=string), word, line, n=1) %>% 
        tail(3) %>% 
        arrange(desc(row_number()))
    trigram <- trigram_rows$word
    log_lambda <- log10(lambda)
    
    pred_rows4 <- ngrams4_stats[word1==trigram[3] & word2==trigram[2] & word3==trigram[1]]
    pred_rows4 <- merge(pred_rows4, co_scores, by.x = "word4", by.y = "word2", all.x = TRUE, all.y = FALSE)
    pred_rows4$co_score[is.na(pred_rows4$co_score)] <- eta_co_score
    pred_rows4$score <- pred_rows4$log_cond_prob + (beta*pred_rows4$co_score)
    pred_rows4 <- pred_rows4[order(-score)][1]
    preds <- data.frame(word=pred_rows4$word4, log_cond_prob = pred_rows4$log_cond_prob, score=pred_rows4$score)
    
    pred_rows3 <- ngrams3_stats[word1==trigram[2] & word2==trigram[1]]
    pred_rows3 <- merge(pred_rows3, co_scores, by.x = "word3", by.y = "word2", all.x = TRUE, all.y = FALSE)
    pred_rows3$co_score[is.na(pred_rows3$co_score)] <- eta_co_score
    pred_rows3$score <- pred_rows3$log_cond_prob + (beta*pred_rows3$co_score)
    pred_rows3 <- pred_rows3[order(-score)][1]
    preds <- rbind(preds, data.frame(word=pred_rows3$word3, log_cond_prob = pred_rows3$log_cond_prob,
                                     score=pred_rows3$score))
    
    pred_rows2 <- ngrams2_stats[word1==trigram[1]]
    pred_rows2 <- merge(pred_rows2, co_scores, by.x = "word2", by.y = "word2", all.x = TRUE, all.y = FALSE)
    pred_rows2$co_score[is.na(pred_rows2$co_score)] <- eta_co_score
    pred_rows2$score <- pred_rows2$log_cond_prob + (beta*pred_rows2$co_score)
    pred_rows2 <- pred_rows2[order(-score)][1]
    preds <- rbind(preds, data.frame(word=pred_rows2$word2, log_cond_prob = pred_rows2$log_cond_prob,
                                     score=pred_rows2$score))
    
    pred_rows1 <- ngrams1_stats
    pred_rows1 <- merge(pred_rows1, co_scores, by.x = "word1", by.y = "word2", all.x = TRUE, all.y = FALSE)
    pred_rows1$co_score[is.na(pred_rows1$co_score)] <- eta_co_score
    pred_rows1$score <- pred_rows1$log_cond_prob + (beta*pred_rows1$co_score)
    pred_rows1 <- pred_rows1[order(-score)][1]
    preds <- rbind(preds, data.frame(word=pred_rows1$word1, log_cond_prob = pred_rows1$log_cond_prob,
                                     score=pred_rows1$score))
    
    preds$score <- as.numeric(preds$score)
    if(sum(is.na(preds$score))==nrow(preds)) {
        pred_word <- preds[which(preds$log_cond_prob == max(preds$log_cond_prob, na.rm = TRUE)),]$word
    } else {
        pred_word <- preds[which(preds$score == max(preds$score, na.rm = TRUE)),]$word
    }
    pred_word[1]
}

str <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
predict_next_word(str)
system.time(predict_next_word(str))

str <- "when"
predict_next_word(str)
```

Great! However, this system takes considerably more time than the stupid backoff without co-occurrences. Now, does this new method actually produce any difference in accuracy? We will look into this question below.

### 2. Tuning Beta
We will replicate the tuning method used before, using the CV set this time. We will only use about 1600 entries from the CV set to tune, since it takes about 1 second to process every 10 entries.
```{r cv_data}
cv_lines <- readRDS(here("data/en_US/cv_en_US_lines.Rds"))

cv_sentences <- unnest_sentences(cv_lines, sentence, line) %>% 
    mutate(type_id = factor(type_id), sentence_id = row_number())

cv_words <- unnest_ngrams(cv_sentences, word, sentence, n=1)

cv_sentences <- cv_words %>% 
    group_by(sentence_id) %>% 
    mutate(sentence = paste(word, collapse = " ")) %>%
    select(sentence_id, sentence) %>%
    distinct()

cv_data <- cv_sentences %>%
    mutate(
        string = gsub("(.*) .*$", "\\1", sentence), 
        pred_word = gsub(".* (.*)$", "\\1", sentence)
    ) %>%
    select(-sentence)
head(cv_data)
dim(cv_data)

rm(list=rm("cv_lines","cv_sentences","cv_words"))

set.seed(readLines(here("scripts/seed.txt")))
cv_sample <- cv_data[sample(1:nrow(cv_data),nrow(cv_data)*0.02),]
head(cv_sample)
dim(cv_sample)
```

```{r get_acc}
get_model_accuracy <- function(b) {
    acc <- sum(sapply(cv_sample$string, predict_next_word, beta=b)==cv_sample$pred_word)/nrow(cv_sample)
    acc
}

get_model_accuracy(0)
get_model_accuracy(0.2)
get_model_accuracy(0.4)
get_model_accuracy(0.6)
get_model_accuracy(0.8)
get_model_accuracy(1)
```
We see that a beta of 0.6 works best here. Let's tune this even further with some finer increments around 0.6.
```{r beta_tune}
get_model_accuracy(0.5)
get_model_accuracy(0.55)
get_model_accuracy(0.65)
get_model_accuracy(0.7)
```

Based on this further tuning, we see the best accuracy with beta=0.55. Thus, this beta value will be used for the final model.
```{r tuned_beta}
tuned_beta <- 0.55
```

## Model Evaluation
You will note that our final model has a seemingly abysmal accuracy, and while this is somewhat true, we calculated the accuracy in a way that makes our model inherently undercount the true accuracy. In reality, the out-of-sample accuracy should be estimated not with sentence completion, but with the prediction of entire sentences, word-by-word. It is likely that, in these cases, certain common structures and connectors (typically stop words) would be easily predicted, such as "the" and "a" in certain 4- or 3-grams. Since no sentence ends with such words, the accuracy predicted is likely inherently lower than the true out-of-sample accuracy. For example, let's run the prediction for every word in the sentence "This is a random sentence.".
```{r full_sentence_predict}
predict_next_word("")
predict_next_word("This")
predict_next_word("This is")
predict_next_word("This is a")
predict_next_word("This is a random")
```
Now, we see that most of the predictions were wrong, but the accuracy still ended up being 40%. The words "is" and "a" were predicted correctly, but these would never occur at the end of sentences. Thus, we must specify that the model has a 10.8% sentence *completion* accuracy, but the true accuracy has yet to be calculated.

## Save Model Data
Now, we will save the data used in our model to our file system to pull the model later in the app when necessary.
```{r save_data}
saveRDS(ngrams1_stats, here("model/data/unigram_stats.Rds"))
saveRDS(ngrams2_stats, here("model/data/bigram_stats.Rds"))
saveRDS(ngrams3_stats, here("model/data/trigram_stats.Rds"))
saveRDS(ngrams4_stats, here("model/data/quadrigram_stats.Rds"))
saveRDS(co, here("model/data/co_matrix.Rds"))
file.create(here("model/data/lambda.txt"))
write_lines(tuned_lambda, here("model/data/lambda.txt"))
file.create(here("model/data/beta.txt"))
write_lines(tuned_beta, here("model/data/beta.txt"))
file.create(here("model/data/seed.txt"))
write_lines(readLines(here("scripts/seed.txt")), here("model/data/seed.txt"))
```
The final function to be used for prediction will be formalized in a final_model script.